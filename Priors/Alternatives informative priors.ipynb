{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fischer Information\n",
    "\n",
    "We should first define Fischer Information, because we will use it at this document. The Fisher information (for one parameter) is denoted as $$I(\\theta) = E[(\\dfrac{d}{d\\theta}log(f(X|\\theta)))^{2}] $$\n",
    "    where the expectation is taken with respect to X which has a PDF $f(x|\\theta)$. This quality is useful in obtaining estimators for $\\theta$ with good properties, such as low variance. It is also the basis for Jeffrey's prior. \n",
    "    \n",
    "**Example**: Let $X|\\theta \\sim N(\\theta, 1)$. Then we have:\n",
    "$$f(x|\\theta) = \\dfrac{1}{\\sqrt{2\\pi}}exp[-\\dfrac{1}{2}(x-\\theta)^{2}] $$\n",
    "$$log(f(x|\\theta)) = -\\dfrac{1}{2}log(2\\pi)-\\dfrac{1}{2}(x-\\theta)^{2}$$\n",
    "$$\\dfrac{d}{d\\theta}log(f(x|\\theta)) = -\\dfrac{2}{2}(x-\\theta)(-1) = x - \\theta$$\n",
    "$$(\\dfrac{d}{d\\theta}log(f(x|\\theta)))^{2} = (x - \\theta)^{2}$$\n",
    "\n",
    "and so $I(\\theta) = E[(X - \\theta)^{2}] = Var(X) = 1$.\n",
    "    \n",
    "# Non-informative priors\n",
    "\n",
    "So far, we've seen examples of choosing priors that contain a significant amount of information. You've also seen some examples of choosing priors where we're attempting to not put too much information in to keep them vague. \n",
    "\n",
    "Another approach is referred to as objective Bayesian statistics or inference where we explicitly try to minimize the amount of information that goes into the prior. \n",
    "\n",
    "## First case\n",
    "This is an attempt to have the data have maximum influence on the posterior which mention further this as **non-informative priors**. \n",
    "\n",
    "For example, let's go back to coin flipping or data comes from Bernoulli distribution with unknown parameter theta. How do we minimize our par information in $\\theta$? One obvious intuitive approach is to say that all values of theta are equally likely. So we could have a prior for $\\theta \\sim U[0, 1]$. Saying all values of theta are equally likely seems like it would have no information in it. \n",
    "\n",
    "Saying all values of theta are equally likely seems like it would have no information in it. \n",
    "\n",
    "The **effective sample size** of a beta prior is the sum of it's two parameters. So in this case it has an effective sample size of two. This is equivalent to data, with one head and one tail already in it. \n",
    "\n",
    "So this is not a completely non informative prior. \n",
    "\n",
    "We could think about a prior that has less information. \n",
    "\n",
    "For example, a beta 1/2, 1/2. This would have only half as much information as an effective sample size of just one. We can take this even further. Think about something like a beta 0.001, 0.001. This would have much less information, have a sample fairly close to 0. In this case, the data would determine the posterior and there would be very little influence from the prior.\n",
    "\n",
    "Can we go even further? In fact we can, we can think of the limiting case. Something that we can think of as a $Beta(0,0)$. What would that look like? $$ f(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}$$\n",
    "\n",
    "\n",
    "**This is not a proper density. If you integrate this over 0 to 1, you'll get an infinite integral, so it's not a true density in the sense of integrating to 1. **There's no way to normalize it, it has an infinite integral. This is what we refer to as an **improper prior**. \n",
    "\n",
    "\n",
    "It's improper in the sense that it doesn't have a proper density. But it's not necessarily improper, in the sense that we can't use it. If we collect data, we use this prior and as long as we observe at least one head and at least one tail. Or one's success and one's failure then we can get a posterior:\n",
    "$$f(\\theta|y) \\propto \\theta^{y-1}(1-\\theta)^{n-y-1} \\text{ , }\\sim Beta(y, n-y)$$. \n",
    "\n",
    "\n",
    "Its posterior mean will be: $\\hat{\\theta} = \\dfrac{y}{n}$, which should be recognised as the **MLE**.\n",
    "\n",
    "So by using this improper prior, we get a posterior which gives us point estimates exactly the same as the frequentest approach. \n",
    "\n",
    "\n",
    "But in this case, we can also think of having a full posterior. And so if we want to make interval statements, probability statements, we can actually find an interval and say that there's 95% probability that $\\theta$ is in this interval. Which is not something you can do under the frequentest approach even though we may get the exact same interval. \n",
    "\n",
    "\n",
    "Key concepts here that I want to state in terms of using improper priors. \n",
    "\n",
    "    1) improper priors are okay as long as the posterior itself is proper. There may be some mathematical things that need to be checked and you may need to have certain restrictions on the data. In this case, we need to make sure that we observe at least one head and one tail to get a proper posterior. But as long as the posterior is proper, we can go forward and do Bayesian inference even with an improper prior. \n",
    "    \n",
    "    2)  For many problems there does exist a prior, typically an improper prior. That will lead to the same point estimates as you would get under the frequentest paradigm. So we can get very similar results, results that are fully dependent on the data, under the Bayesian approach. \n",
    "\n",
    "\n",
    "## Second case\n",
    "\n",
    "Let: $Y_{i} \\sim N(\\mu, \\sigma^{2})$\n",
    "\n",
    "** Known $\\sigma$**\n",
    "\n",
    "We assume that $\\sigma$ is known. We take a vague prior : $\\mu \\sim N(0, 10000^{2})$. That would just spread things out across the real line. You can take a wide variety of possible values. That would be fairly non informative across a lot of possibilities. \n",
    "\n",
    "We can then think about taking the limit. What happens if we let the variance go to infinity? In that case, we're basically spreading out this distribution across the entire real line. And so we could say, we have a density which is proportional to what? It's just constant across the whole real line. \n",
    "\n",
    "Clearly, this is an improper prior because if you integrate the real line you get an infinite answer. However, if we go ahead and plug this into finding a posterior $$f(\\mu|y)\\propto f(\\mu|y)f(y)$$$$\\propto \\exp{\\{-\\dfrac{1}{2\\sigma^{2}}\\sum(y_{i}-\\mu)^2}\\}$$ $$\\propto exp{\\{-\\dfrac{1}{2\\dfrac{\\sigma^{2}}{n}}(\\mu - \\bar{y})^{2}}\\}$$.\n",
    "$$\\mu|y \\sim N(\\bar{y}, \\dfrac{\\sigma^{2}}{n})$$\n",
    "\n",
    "** Unknown $\\sigma$**\n",
    "\n",
    "The standard non informative prior is :$$f(\\sigma^{2})\\propto \\dfrac{1}{\\sigma^{2}} \\text{ , } \\Gamma^{-1}(0,0)$$ This is an improper prior and it's uniform on the log scale of sigma squared. \n",
    "\n",
    "Posterior for $\\sigma^{2}$:\n",
    "$$\\sigma^{2}|y \\sim \\Gamma^{-1}(\\dfrac{n-1}{2}, \\dfrac{1}{2}\\sum(y_{i}-\\bar{y})^{2})$$\n",
    "\n",
    "# Jeffrey's Prior \n",
    "\n",
    "Choosing a uniform prior depends upon the particular parameterization. For example, $y_{i} \\sim N(\\mu, \\sigma^{2})$\n",
    "\n",
    "Suppose, I used a prior which is uniform on the log scale for $\\sigma^{2}$, so $f(\\sigma^{2}) \\propto \\dfrac{1}{\\sigma^{2}}$.\n",
    "\n",
    "\n",
    "Suppose somebody else decides, they just want to put a uniform prior on $\\sigma^{2}$ itself, $f(\\sigma^{2})\\propto 1$. These are both uniform on certain scales, or certain parameterizations, but they are different priors. \n",
    "\n",
    "So when we compute the posteriors, we will get different posteriors. The key thing is that **uniform priors are not invariant with respect to transformation**. Depending upon how you parameterize the problem, you can get different answers by using a uniform prior. \n",
    "\n",
    "**Jeffreys prior is one attempt to round to this **\n",
    "\n",
    "$$f(\\theta) \\propto \\sqrt{I(\\theta)}$$\n",
    "\n",
    "The Jeffreys prior is exactly the prior we have seen before. It's uniform for $\\mu$, and then for $\\sigma^{2}$ it's uniformed on the log scale. This prior will then be invariant transformation will be putting the same information into the prior. Even if we use a different parameterization for the normal. \n",
    "\n",
    "In the example of $Y_{i} \\sim B(\\theta) \\text {       } f(\\theta) \\propto \\theta^{-\\dfrac{1}{2}}(1-\\theta)^{-\\dfrac{1}{2}} \\sim Beta(\\dfrac{1}{2},\\dfrac{1}{2})$. \n",
    "\n",
    "This is a rare example where the Jeffreys prior turns out to be a proper prior. You'll note that this prior actually does have some information in it. It's equivalent to an **effective sample size of one data point**. However this information will then be the same, not depending on the prioritization we use. This case we have $\\theta$ as a probability. But another alternative, used in probabilities calculations, sometimes we model things on a logistics scale. And in that case, we can transfer everything and using the Jeffreys prior, we'll maintain the exact same information. \n",
    "\n",
    "Other possible approaches to objective basing inference include priors such as reference priors and maximum entropy priors. \n",
    "\n",
    "Finally, I'd like to mention a related concept which is empirical basing analysis. The idea in empirical base is that you use the data to help inform your prior, such as using the mean of the data to set the mean of the prior distribution. This approach often leads to reasonable point estimates in your posterior. However, it's sort of cheating, because you're using the data twice. And as a result, it may lead to improper uncertainty estimates. \n",
    "\n",
    "**Question **\n",
    "\n",
    "Jeffreys priors are \"transformation invariant\" in the sense that if we calculate the Jeffreys proor for $\\theta$ and then reparameterize to use $\\phi = g(\\theta)$, we get the same result as if we had first reparameterized adn then found the Jeffrey's prior for $\\phi$. Why might this property be desirable? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "Different investigators might parameterize a problem in different ways. Using the Jeffreys prior that ensures they both obtain the same answer. \n",
    "\n",
    "\n",
    "\n",
    "# Review\n",
    "\n",
    "** Question 1 **\n",
    "\n",
    "Suppose we flip a coin five times to estimate θ, the probability of obtaining heads. We use a Bernoulli likelihood for the data and a non-informative (and improper) Beta(0,0) prior for θ. We observe the following sequence: (H, H, H, T, H).\n",
    "\n",
    "Because we observed at least one H and at least one T, the posterior is proper. What is the posterior distribution for θ?\n",
    "\n",
    "**Answer 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4"
      ],
      "text/latex": [
       "4"
      ],
      "text/markdown": [
       "4"
      ],
      "text/plain": [
       "[1] 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a <- 0\n",
    "b <- 0 \n",
    "y <- c(1, 1, 1, 0, 1)\n",
    "n <- length(y)\n",
    "post_a <- a + sum(y); post_a\n",
    "post_b <- b + n - sum(y) ; post_b\n",
    "#Beta(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 2**\n",
    "\n",
    "Continuing the previous question, what is the posterior mean for θ? \n",
    "\n",
    "**Answer 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.8"
      ],
      "text/latex": [
       "0.8"
      ],
      "text/markdown": [
       "0.8"
      ],
      "text/plain": [
       "[1] 0.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "post_a/(post_a + post_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the thermometer calibration problem.\n",
    "\n",
    "Assume a normal likelihood with unknown mean θ and known variance σ^{2}=0.25. Now use the non-informative (and improper) flat prior for θ across all real numbers. This is equivalent to a conjugate normal prior with variance equal to $\\infty$.\n",
    "\n",
    "** Question 3**\n",
    "\n",
    "You collect the following n=5 measurements: (94.6, 95.4, 96.2, 94.9, 95.9). What is the posterior distribution for θ?\n",
    "\n",
    "**Answer 3**\n",
    "\n",
    "N(95.4, 0.25)\n",
    "\n",
    "Recall from the lesson that with a flat prior on θ, the posterior distribution is $N(\\bar{y},\\sigma^{2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "95.4"
      ],
      "text/latex": [
       "95.4"
      ],
      "text/markdown": [
       "95.4"
      ],
      "text/plain": [
       "[1] 95.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "measurements <- c(94.6, 95.4, 96.2, 94.9, 95.9)\n",
    "n <- length(measurements)\n",
    "mean <- mean(measurements); mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 4**\n",
    "\n",
    "Plot the Jeffreys prior for a Bernoulli/binomial success probability p.\n",
    "\n",
    "**Answer 4**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nACFBgJAgQEgQICQIEBIECAkChAQB/wH+AhYk6hfWDgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p=seq(from=0,to=1,by=.01)\n",
    "plot(p,dbeta(p,0.5,0.5),type=\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 5** \n",
    "\n",
    "Scientist A studies the probability of a certain outcome of an experiment and calls it θ. To be non-informative, he assumes a Uniform(0,1) prior for θ.\n",
    "\n",
    "Scientist B studies the same outcome of the same experiment using the same data, but wishes to model the odds $\\phi=\\dfrac{\\theta}{1-\\theta}$. Scientiest B places a uniform distribution on  $\\phi$. If she reports her inferences in terms of the probability $\\theta$, will they be equivalent to the inferences made by Scientist A?\n",
    "\n",
    "**Answer 5**\n",
    "\n",
    "No, they did not use the Jeffreys prior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
